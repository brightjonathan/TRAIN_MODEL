{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# BYTE PAIR ENCODING (BPE) --- TOKENIZATION"
      ],
      "metadata": {
        "id": "cgiyBeYqkTtg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Load the sequence"
      ],
      "metadata": {
        "id": "tmlvKCHqkh-g"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "with open(\"/content/TRAIN_MODEL/output/combined_text.txt\", \"r\", encoding=\"utf-8\") as f:\n",
        "    text_sequence = f.read()\n",
        "\n",
        "len(text_sequence)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nkgfo5idkn0M",
        "outputId": "b31a75ab-7fba-4dfa-8e90-b7130a67a550"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1930710"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "BPE algorithm\n",
        "\n",
        "I am using the minBPE repository to tokenize the sequence of text."
      ],
      "metadata": {
        "id": "kJ6HNLsrmuio"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import sys\n",
        "sys.path.append('..')"
      ],
      "metadata": {
        "id": "dC2iZbWmm0oL"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#The BPE algorithm works by iteratively merging the most frequent pairs of bytes in the text until the desired vocabulary size is reached.\n",
        "from minbpe.basic import BasicTokenizer\n",
        "\n",
        "tokenizer = BasicTokenizer()\n",
        "tokenizer.train(text_sequence, vocab_size=1024)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pHUEvqRSnNQh",
        "outputId": "4cb1e631-ddf4-4963-9122-ac461ed4709c"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 768/768 [04:30<00:00,  2.84it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "vocab = tokenizer.vocab\n",
        "vocab"
      ],
      "metadata": {
        "collapsed": true,
        "id": "qndw8rsRsEAE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Testing the tokenizer.\n",
        "\n",
        "tokenizer.encode(\"hw far\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jD47CPeTsFUY",
        "outputId": "6d9d2ab5-20c4-43a0-cdb2-7ff391a18708"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[104, 423, 102, 296]"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer.decode([104, 423, 102, 296])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "DvpHe9AlsibZ",
        "outputId": "826af5a5-82ae-4f13-9ab9-7d3f443f7ed3"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'hw far'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Adding special tokens to the vocabulary. These tokens are going to be used a lot in the fine-tuning step.\n",
        "\n",
        "max_vocab_id = list(tokenizer.vocab.keys())[-1]\n",
        "tokenizer.special_tokens = {\n",
        "    \"<|startoftext|>\": max_vocab_id + 1,\n",
        "    \"<|separator|>\": max_vocab_id + 2,\n",
        "    \"<|endoftext|>\": max_vocab_id + 3,\n",
        "    \"<|unk|>\": max_vocab_id + 4,\n",
        "    \"<|padding|>\": max_vocab_id + 5,\n",
        "}"
      ],
      "metadata": {
        "id": "l1p-x7EFs57g"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#I have more than 669K tokens for training and validation. Pretty good for a start\n",
        "len(tokenizer.encode(text_sequence))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Fkdn2T0_tpIa",
        "outputId": "85c33aa0-2e73-401f-f397-d7f45322233f"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "669641"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#saving the tokenizaton\n",
        "tokenizer.save(file_prefix=\"/content/TRAIN_MODEL/output/my_tokenizer\")"
      ],
      "metadata": {
        "id": "1PJDi3PqvShk"
      },
      "execution_count": 24,
      "outputs": []
    }
  ]
}